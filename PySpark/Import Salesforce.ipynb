{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar datos de Salesforce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DEV/Utils/LakehouseFunctions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token():\n",
    "    \"\"\"\n",
    "    Genera un access token para conectarse a la plataforma de Veeva\n",
    "\n",
    "    Devuelve:\n",
    "    response (json): JSON con la respuesta a la llamada de la API. Si la llamada es satisfactoria, dentro del JSON se encuentra el access token en la clave \"access_token\" \n",
    "    \"\"\"\n",
    "    # Ref: https://developer.salesforce.com/docs/atlas.en-us.api_rest.meta/api_rest/intro_understanding_oauth_endpoints.htm\n",
    "    # Auth URL\n",
    "    df_veeva = pd.read_sql(\"SELECT * FROM Origins WHERE Name = 'Veeva'\", engine_conn)\n",
    "    endpoint = df_veeva[\"Endpoint\"][0]\n",
    "    auth_url = f\"{endpoint}/services/oauth2/token\"\n",
    "\n",
    "    consumer_key     = dbutils.secrets.get(keyvault_name, df_veeva[\"Username\"][0]) \n",
    "    consumer_secret  = dbutils.secrets.get(keyvault_name, df_veeva[\"Secret\"][0])\n",
    "\n",
    "    # POST request for access token\n",
    "    response = requests.post(auth_url, data = {\n",
    "                        'grant_type': 'client_credentials',\n",
    "                        'client_id':consumer_key,\n",
    "                        'client_secret':consumer_secret\n",
    "                        })\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_select_all_query(response, table_source, filter_date = True, n_days=7, filter_where=\"\", column_name = \"LastModifiedDate\", version='v48.0'):\n",
    "    \"\"\"\n",
    "    Construye la query para Salesforce según su lógica. Dependiendo de los parámetros que se pasan a la función esta se construye de diferente manera.\n",
    "\n",
    "    Parametros:\n",
    "    response (json): JSON de la respuesta de la función \"get_access_token\". Contiene el token de acceso, entre otros.\n",
    "    table_source (string): Contiene el nombre de la tabla origen de Salesforce de donde se van a obtener los datos.\n",
    "    filter_date (bool): Valor por defecto -> True. Condición que hace que la query tenga en cuenta los últimos ndays.\n",
    "    n_days (int): Valor por defecto -> 7. Número de días a traer en la query. Necesita que filter_date esté en True.\n",
    "    filter_where (string): Valor por defecto -> \"\". Su función es especificar la clausula WHERE.\n",
    "    column_name (string): Valor por defecto -> \"LastModifiedDate\". Permite escoger sobre que columna se compara la fecha.\n",
    "    version (string): Valor por defecto -> \"v48.0\". Especifica la versión que aplicar en la query.\n",
    "\n",
    "    Devuelve:\n",
    "    query (string): String construida segun todo lo que se ha pasado por parametro.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Read access token\n",
    "    json_res = response.json()\n",
    "    access_token = json_res['access_token']\n",
    "    auth = {'Authorization':'Bearer ' + access_token}\n",
    "    instance_url = json_res['instance_url']\n",
    "\n",
    "    query = '/services/data/'+version+'/sobjects/'+table_source+'/describe/'\n",
    "    url = instance_url + query\n",
    "    req = requests.get(url, headers=auth).json()\n",
    "    query = '/services/data/'+version+'/query/?q=SELECT+'\n",
    "    i = 0\n",
    "    for f in req[\"fields\"]:\n",
    "        query = query+f[\"name\"]+',+'\n",
    "        i+=1\n",
    "    query = query[:-2]+\"+FROM+\"+table_source\n",
    "\n",
    "    if filter_date and filter_where:  query = query+\"+WHERE+(+\"+column_name+\"+=+LAST_N_DAYS:\"+str(n_days)+\"+)\"+\"+AND+(+\"+filter_where+\"+)\"\n",
    "    elif filter_date and not filter_where: query = query+\"+WHERE+\"+column_name+\"+=+LAST_N_DAYS:\"+str(n_days)\n",
    "    elif not filter_date and filter_where: query = query+\"+WHERE+\"+filter_where\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_veeva(response, query, table_destination, columns=[]):\n",
    "    \"\"\"\n",
    "    Obtiene los datos de Veeva usando el access_token y la query construida en \"build_select_all_query\".\n",
    "\n",
    "    Parametros:\n",
    "    response (json): JSON de la respuesta de la función \"get_access_token\". Contiene el token de acceso, entre otros.\n",
    "    query (string): String construida segun todo lo que se ha pasado por parametro en la función \"build_select_all_query\".\n",
    "    table_destination (string): Nombre de la tabla donde se van a guardar los datos importados de Veeva.\n",
    "    columns (lista de strings): Valor por defecto -> Vacía. Define las columnas que se quieren obtener del dataframe. Si no se pasa el parametro, se obtienen todas las columnas.\n",
    "\n",
    "    Devuelve:\n",
    "    Nada\n",
    "    \"\"\"\n",
    "    \n",
    "    access_token = response.json()['access_token']\n",
    "    auth = {'Authorization':'Bearer ' + access_token}\n",
    "    instance_url = response.json()['instance_url']\n",
    "    \n",
    "    begin_time=time.time()\n",
    "    url = instance_url + query #Create and execute the query\n",
    "    req = requests.get(url, headers=auth).json()\n",
    "    df = pd.DataFrame(req[\"records\"])\n",
    "    if 'attributes' in df: df=df.drop(\"attributes\", axis=1)\n",
    "    if columns: df=df[columns]\n",
    "    contador=len(df.index)\n",
    "    if not df.empty:\n",
    "        df = df.applymap(str).replace(to_replace='None', value= '')\n",
    "        write_raw_data(spark.createDataFrame(df), table_destination)\n",
    "        #df=df.replace(to_replace='None', value= None, regex=True)\n",
    "        #df=df.replace(to_replace='null', value= '')\n",
    "        #df = df.replace('', pd.NA, regex=True)\n",
    "        #sparkDF=sparkDF.replace('None',lit(None).cast(StringType()))\n",
    "        #display(sparkDF)\n",
    "        #df.to_sql(table_destination, conn, if_exists=\"append\", index=False)\n",
    "        print(\"\\n \"+table_destination+\": Insertando \"+str(contador)+\" registros... (TOTAL: \"+str(contador)+\" registros | TIEMPO: \"+str(format(time.time()-begin_time, \".2f\"))+\"s)\")\n",
    "\n",
    "    while \"nextRecordsUrl\" in req:\n",
    "        nru = instance_url + req[\"nextRecordsUrl\"]\n",
    "        req = requests.get(nru, headers=auth).json()\n",
    "        df = pd.DataFrame(req[\"records\"])\n",
    "        if 'attributes' in df: df=df.drop(\"attributes\", axis=1)\n",
    "        if columns: df=df[columns]\n",
    "        n_reg=len(df.index)\n",
    "        if not df.empty:\n",
    "            df = df.applymap(str).replace(to_replace='None', value= '')\n",
    "            write_raw_data(spark.createDataFrame(df), table_destination)\n",
    "            contador+=n_reg\n",
    "            if contador % 1000 == 0:\n",
    "                print(\"\\n \"+table_destination+\": Insertando \"+str(n_reg)+\" registros... (TOTAL: \"+str(contador)+\" registros | TIEMPO: \"+str(format(time.time()-begin_time, \".2f\"))+\"s)\")\n",
    "                \n",
    "    print(\"\\n Tabla \"+table_destination+\" insertada correctamente (\"+str(contador)+\" registros)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query a BBDD Catálogo para obtener los datasets y origenes necesarios\n",
    "df_datasets = pd.read_sql(\"SELECT Datasets.Id as DatasetId, Datasets.Name as DatasetName, Query, Keys, ChangeTrackingVersion, TableName FROM Origins LEFT JOIN Datasets on Origins.Id = Datasets.OriginId WHERE Origins.Name = 'Veeva' and IsActive = 1\", engine_conn)\n",
    "\n",
    "# Llamada a la función para obtener el access_token\n",
    "response = get_access_token()\n",
    "\n",
    "# Definición de variables para la función que crea la query (Carga incremental y datos de los últimos 20 días)\n",
    "Incremental=True\n",
    "n_days=20\n",
    "\n",
    "# Se obtienen datos de cada uno de los datasets obtenidos en catálogo\n",
    "for _, table in df_datasets.iterrows():\n",
    "    try:\n",
    "        # Se borran los datos de raw para tener una subida limpia (Incremental)\n",
    "        delete_raw_data(table['TableName'])\n",
    "\n",
    "        # Se construye la query con la información del dataset y los parametros definidos antes\n",
    "        query =  build_select_all_query(response,table['DatasetName'], Incremental, n_days)\n",
    "        \n",
    "        # Se obtienen los datos usando la query construida\n",
    "        get_data_from_veeva(response, query, table['TableName'])\n",
    "\n",
    "        # Se guardan los datos importados en raw en la tabla final de forma incremental\n",
    "        write_standardized_data(table['TableName'], Incremental, keys=table['Keys'])\n",
    "        print(\"Tabla cargada correctamente: \", table['TableName'])\n",
    "    except Exception as e:\n",
    "        print(\"Error writing table: \"+table['TableName'], str(e))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
